flink:
  deployment: #注意这里的参数一定能要flink启动支持的参数(因为在启动参数解析时使用了严格模式,一个不识别会停止解析),详情和查看flink官网,否则会造成整个参数解析失败,最明细的问题的找不到jar文件
    dynamic: ##这里放动态熟悉... -yD key=value
    resource: ## 这里放启动资源相关配置...s
      yarnname: dp-hopsonone-park-clean            # Set a custom name for the application on YARN
      class: cn.com.hopson.parkclean.app.ShParkCleanApp                             # main class
      detached: false                    # -d   (If present, runs the job in detached mode)
      jobmanager: yarn-cluster                      # -m   (Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration.)
      shutdownOnAttachedExit:           # -sae (If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C.)
      yarnapplicationType:              # -yat (Set a custom application type for the application on YARN)
      yarnjobManagerMemory: 1024M       # -yjm (Memory for JobManager Container with optional unit (default: MB))
      yarncontainer:                    # -yn  (Number of YARN container to allocate (=Number of Task Managers)
      yarnnodeLabel:                    # -ynl (Specify YARN node label for the YARN application)
      yarnqueue:                        # -yqu (Specify YARN queue)
      yarnslots: 1                      # -ys  (Number of slots per TaskManager)
      yarntaskManagerMemory: 4096M      # -ytm
  time.characteristic: EventTime
  parallelism: 1
  checkpoint:
    enable: true
    interval: 1000
    mode: EXACTLY_ONCE

mysql:
  driverClassName: com.mysql.jdbc.Driver
  jdbcUrl: jdbc:mysql://localhost:3306/test
  username: root
  password: 123322242
  batch.size: 100

influx:
  mydb:
    jdbcUrl: http://test9:8086
    #username: admin
    #password: admin

# hbase
hbase:
  zookeeper.quorum: test-hadoop-1,test-hadoop-2,test-hadoop-6
  zookeeper.property.clientPort: 2181

#clickhouse
clickhouse.sink:
  driverClassName:
  jdbcUrl: http://192.168.0.100:8123
  username: default
  password: 123322242
  threshold:
    bufferSize: 100
    numWriters: 3
    queueCapacity: 100
    timeout: 1000
    successCode: 200
    retries: 10 #一条记录当插入失败时重试的最大次数
  failover:
    storage: mysql #kafka,hbase,hdfs
    mysql: # 保存类型为MySQL,将失败的数据保存到MySQL
      jdbcUrl: jdbc:mysql://localhost:3306/test
      username: root
      password: 123322242
    kafka:
      topic: bigdata
      bootstrap.servers: localhost:9091,localhost:9092,localhost:9093
    hbase:
      zookeeper.quorum: localhost
      zookeeper.property.clientPort: 2181
    hdfs:
      namenode: hdfs://localhost:8020 # namenode rpc address and port, e.g: hdfs://hadoop:8020 , hdfs://hadoop:9000
      user: benjobs # user
      path: /clickhouse/failover # save path
      format: yyyy-MM-dd

http.sink:
  threshold:
    numWriters: 3
    queueCapacity: 10000 #队列最大容量,视单条记录大小而自行估量队列大小,如值太大,上游数据源来的太快,下游写入数据跟不上可能会OOM.
    timeout: 100 #发送http请求的超时时间
    retries: 3 #发送失败时的最大重试次数
    successCode: 200 #发送成功状态码,这里可以有多个值,用","号分隔
  failover:
    table: record
    storage: mysql #kafka,hbase,hdfs
    mysql: # 保存类型为MySQL,将失败的数据保存到MySQL
      jdbcUrl: jdbc:mysql://localhost:3306/test
      username: root
      password: 123322242
    kafka:
      topic: bigdata
      bootstrap.servers: localhost:9091,localhost:9092,localhost:9093
    hbase:
      zookeeper.quorum: localhost
      zookeeper.property.clientPort: 2181
    hdfs:
      namenode: hdfs://localhost:8020 # namenode rpc address and port, e.g: hdfs://hadoop:8020 , hdfs://hadoop:9000
      user: benjobs # user
      path: /clickhouse/failover # save path
      format: yyyy-MM-dd
