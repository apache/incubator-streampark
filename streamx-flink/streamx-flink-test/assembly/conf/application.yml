flink:
  deployment: #注意这里的参数一定能要flink启动支持的参数(因为在启动参数解析时使用了严格模式,一个不识别会停止解析),详情和查看flink官网,否则会造成整个参数解析失败,最明细的问题的找不到jar文件
    dynamic: ##这里放动态熟悉... -yD key=value
    option: ## 这里放启动资源相关配置...注意 -yn(yarncontainer)参数从1.9.0开始已经过时,不推荐使用,从1.10开始已废除.
      yarnname: FlinkSinkApp            # Set a custom name for the application on YARN
      class: com.streamxhub.flink.test.FlinkSinkApp                             # main class
      detached:                         # -d   (If present, runs the job in detached mode)
      jobmanager: yarn-cluster          # -m   (Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration.)
      shutdownOnAttachedExit:           # -sae (If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C.)
      yarnapplicationType:              # -yat (Set a custom application type for the application on YARN)
      yarnjobManagerMemory: 1024M       # -yjm (Memory for JobManager Container with optional unit (default: MB))
      yarnnodeLabel:                    # -ynl (Specify YARN node label for the YARN application)
      yarnqueue:                        # -yqu (Specify YARN queue)
      yarnslots: 1                      # -ys  (Number of slots per TaskManager)
      yarntaskManagerMemory: 4096M      # -ytm
  watermark:
    time.characteristic: EventTime
    interval: 5000
  parallelism: 1
  checkpoints:
    enable: true
    interval: 1000
    mode: EXACTLY_ONCE
    #dir: hdfs:///

#state.backend
state.backend: rocksdb #保存类型(jobmanager,filesystem,rocksdb)
state.backend.memory: 5242880 #针对jobmanager有效,最大内存
state.backend.async: false # 针对(jobmanager,filesystem)有效,是否开启异步
state.backend.incremental: true #针对rocksdb有效,是否开启增量
#rocksdb config: https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/config.html#rocksdb-configurable-options
#state.backend.rocksdb.block.blocksize:
#....

mysql:
  driverClassName: com.mysql.jdbc.Driver
  jdbcUrl: jdbc:mysql://localhost:3306/test
  username: root
  password: 123322242
  batch.size: 100

influx:
  mydb:
    jdbcUrl: http://test9:8086
    #username: admin
    #password: admin

# hbase
hbase:
  zookeeper.quorum: test-hadoop-1,test-hadoop-2,test-hadoop-6
  zookeeper.property.clientPort: 2181
  zookeeper.session.timeout: 1200000
  rpc.timeout: 5000
  client.pause: 20

#clickhouse
clickhouse.sink:
  driverClassName:
  jdbcUrl: http://192.168.0.100:8123
  username: default
  password: 123322242
  threshold:
    bufferSize: 100
    numWriters: 3
    queueCapacity: 100
    timeout: 1000
    successCode: 200
    retries: 10 #一条记录当插入失败时重试的最大次数
  failover:
    storage: mysql #kafka,hbase,hdfs
    mysql: # 保存类型为MySQL,将失败的数据保存到MySQL
      jdbcUrl: jdbc:mysql://localhost:3306/test
      username: root
      password: 123322242
    kafka:
      topic: bigdata
      bootstrap.servers: localhost:9091,localhost:9092,localhost:9093
    hbase:
      zookeeper.quorum: localhost
      zookeeper.property.clientPort: 2181
    hdfs:
      namenode: hdfs://localhost:8020 # namenode rpc address and port, e.g: hdfs://hadoop:8020 , hdfs://hadoop:9000
      user: benjobs # user
      path: /clickhouse/failover # save path
      format: yyyy-MM-dd

http.sink:
  threshold:
    numWriters: 3
    queueCapacity: 10000 #队列最大容量,视单条记录大小而自行估量队列大小,如值太大,上游数据源来的太快,下游写入数据跟不上可能会OOM.
    timeout: 100 #发送http请求的超时时间
    retries: 3 #发送失败时的最大重试次数
    successCode: 200 #发送成功状态码,这里可以有多个值,用","号分隔
  failover:
    table: record
    storage: mysql #kafka,hbase,hdfs
    mysql: # 保存类型为MySQL,将失败的数据保存到MySQL
      jdbcUrl: jdbc:mysql://localhost:3306/test
      username: root
      password: 123322242
    kafka:
      topic: bigdata
      bootstrap.servers: localhost:9091,localhost:9092,localhost:9093
    hbase:
      zookeeper.quorum: localhost
      zookeeper.property.clientPort: 2181
    hdfs:
      namenode: hdfs://localhost:8020 # namenode rpc address and port, e.g: hdfs://hadoop:8020 , hdfs://hadoop:9000
      user: benjobs # user
      path: /clickhouse/failover # save path
      format: yyyy-MM-dd
# source config....
kafka.source:
  bootstrap.servers: test-hadoop-7:9092,test-hadoop-8:9092,test-hadoop-9:9092
  pattern: hopsonone_park_(.*)_tjd_specilog
  topic: kafka01,kafka02
  group.id: flink_test01
  auto.offset.reset: earliest
  enable.auto.commit: true
  start.from:
    #timestamp: 1591286400000 #指定timestamp,针对所有的topic生效
    offset: # 给每个topic的partition指定offset
      topic: kafka01,kafka02,hopsonone_park_gz_tjd_specilog
      kafka01: 0:182,1:183,2:182 #分区0从182开始消费,分区1从183...
      kafka02: 0:182,1:183,2:182
      hopsonone_park_gz_tjd_specilog: 0:192,1:196,2:196


