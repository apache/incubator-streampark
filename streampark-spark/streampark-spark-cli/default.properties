#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

######################################################
#                                                    #
#               spark process startup.sh             #
#                   user config                      #
#                                                    #
######################################################
#Must be set to execute the full package name of the class
spark.app.main=
spark.app.params=
spark.app.id=123

######################################################
#                                                    #
#                spark config                        #
#                                                    #
######################################################
#Perform cluster setup, no setup, generally use YARN
spark.master=yarn
#YARN Deployment Model;default=cluster
spark.submit.deployMode=cluster
#spark-streaming per batch interval;default=300
spark.batch.duration=300
#Task submission queue for spark on yarn;default=defalut
spark.yarn.queue=default
#spark task name configuration, it is recommended to keep the task name globally unique;This allows for some unique handling based on the name when the design task fails;Not set to use the full name of the class.App
spark.app.name=
#The spark network serialization method, the default is JavaSerializer, which can be used for all types but is slower;The recommended Kryo method is used here,kafka-0.10 must use this method
spark.serializer=org.apache.spark.serializer.KryoSerializer

#++++++++++++++++++++++Driver node related configuration+++++++++++++++++++++++++++
#Driver node uses memory size setting;default=512MB
spark.driver.memory=512MB
#The number of cpu's used by Driver nodes is set;default=1
spark.driver.cores=1
#When driver node build spark-jar and user-jar conflict, the user-supplied one is used first, which is an experimental parameter only valid for cluster mode;default=false
spark.driver.userClassPathFirst=false

#++++++++++++++++++++++Executor node related configuration+++++++++++++++++++++++++
#Number of Executor settings,default=1
spark.executor.instances=1
#Executor uses the number of cpu settings;default=1
spark.executor.cores=1
#Executor uses memory size settings;default=512MB
spark.executor.memory=512MB
#The same as the driver node configuration, but for the executor;default=false
spark.executor.userClassPathFirst=true

#++++++++++++++++++++++++Executor dynamic allocation-related configuration++++++++++++++++++++
#Executor dynamically assigned predecessor services;default=false
spark.shuffle.service.enabled=true
#The port corresponding to the service, which is configured in the yarn-site, is loaded and started by the NodeManager service;default=7337
spark.shuffle.service.port=7337
#Configure whether to enable dynamic resource allocation, this dynamic allocation is for executor, need yarn cluster configuration support dynamic allocation;default=false
spark.dynamicAllocation.enabled=true
#Time to release an idle executor;default=60s
spark.dynamicAllocation.executorIdleTimeout=60s
#Idle release time for executor with cache;default=infinity(No release by default)
spark.dynamicAllocation.cachedExecutorIdleTimeout=-1
#Initialize the number of executors, if you set spark.executor.instances to use whoever is small;default=minExecutors(Not set to use this configuration value)
spark.dynamicAllocation.initialExecutors=1
#executor dynamically allocates the maximum number of allocations that can be made;default=infinity
spark.dynamicAllocation.maxExecutors=60
#Minimum number of executor dynamic shrinkage;default=0
spark.dynamicAllocation.minExecutors=1
#How long does the batch scheduling delay start adding executor;default=1s
spark.dynamicAllocation.schedulerBacklogTimeout=1s
#as above, but for subsequent requests;default=SchedulerBacklogTimeout(Not set to use this configuration value)
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=1s

######################################################
#                                                    #
#             StreamPark-Spark Kafka Source             #
#                   base config                      #
#                                                    #
######################################################
#The configuration behind spark.source.kafka.consume is the standard kafka configuration
#kafka consumption of topics configuration, can be configured multiple, each topic separated by a comma [,]
spark.source.kafka.consume.topics=
#kafka consumer\u7684group id;default=kafka.consumer.001
spark.source.kafka.consume.group.id=kafka.consumer.001
#kafka cluster host and port number, you can configure more than one, each host is separated by a comma [,]
spark.source.kafka.consume.bootstrap.servers=
#Specify the location from which the kafka topic is consumed for the first time. There are two optional values latest[latest location],earliest[earliest location].;default=earliest
spark.source.kafka.consume.auto.offset.reset=earliest
#spark consumption kafka when how to manage offset here are three optional values hbase, redis, kafka each value corresponds to a storage method;default=kafka
spark.source.kafka.offset.store.type=kafka
#Custom method for managing kafka offset in spark, need to specify a custom class name
#spark.source.kafka.offset.store.class=none
#The new version of kafka uses the key serialization method;default=java.Serialization
spark.source.kafka.consume.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
#The latest version of kafka uses the value serialization method;default=java.Serialization
spark.source.kafka.consume.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
#Get the maximum length of data at a time, the size of this value needs to be supported by the kafka server;default=10485760
spark.source.kafka.consume.max.partition.fetch.bytes=10485760
#Get the maximum wait time for a data request;default=3000
spark.source.kafka.consume.fetch.max.wait.ms=3000

######################################################
#                                                    #
#              StreamPark-Spark Redis Sink              #
#                   base config                      #
#                                                    #
######################################################
#StreamPark redis sink requires several configurations
#redis host
spark.sink.redis.host=
#redis port
spark.sink.redis.port=6379
#redis database
spark.sink.redis.db=0
#redis connection timeout
spark.sink.redis.timeout=30

######################################################
#                                                    #
#                StreamPark-Spark Monitor               #
#              Congestion base config                #
#                                                    #
######################################################
#default=0
spark.monitor.congestion.batch=0
#How many batches are stacked and then kill the task, the default is 0 without kill, with the automatic task restart function can effectively restart the stacked tasks to restore;default=0
spark.monitor.suicide.batch=0
#zookeeper address
spark.monitor.zookeeper=zookeeper://127.0.0.1:2181
#The api address of the nailbot to send messages, need the full path starting from http
spark.monitor.dingding.url=https://oapi.dingtalk.com/robot/send?access_token=d4d19790b4d4b83bfbeeb9f67e75ed5b1c2e3a40968e9d908df7c691c0f78afe
#To @'s cell phone number
spark.monitor.dingding.user=

